# ‚öñÔ∏è Gender Biases in Automatic Evaluation Metrics for Image Captioning

<div align="center">
  <b>
    Haoyi‚ÄØQiu<sup>1</sup>, Zi-Yi Dou<sup>1</sup>, Tianlu Wang<sup>2</sup>, Asli Celikyilmaz<sup>2</sup>, Nanyun‚ÄØPeng<sup>1</sup>
  </b>
  <br>
  <sup>1</sup>UCLA, <sup>2</sup>Meta FAIR
  <br><br>
  <a href="https://arxiv.org/pdf/2305.14711"><img src="https://img.shields.io/badge/Paper-arXiv-orange"></a>
</div>


## ü´ß Overview

Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have demonstrated decent correlations with human judgments in various language generation tasks. However, their impact on fairness remains largely unexplored. It is widely recognized that pretrained models can inadvertently encode societal biases, thus employing these models for evaluation purposes may inadvertently perpetuate and amplify biases. 

For example, an evaluation metric may favor the caption ‚Äúa woman is calculating an account book‚Äù over ‚Äúa man is calculating an account book,‚Äù even if the image only shows male accountants. 

In this work:
- We conduct a systematic study of gender biases in model-based automatic evaluation metrics for image captioning tasks. 
- We curate a dataset comprising _profession_, _activity_, and _object_ concepts associated with stereotypical gender associations. 
- We demonstrate the negative consequences of using these biased metrics, including the inability to differentiate between biased and unbiased generations, as well as the propagation of biases to generation models through reinforcement learning. 
- We present a simple and effective way to mitigate the metric bias without hurting the correlations with human judgments. Our dataset and framework lay the foundation for understanding the potential harm of model-based evaluation metrics, and facilitate future works to develop more inclusive evaluation metrics.


## üìö PAO-EvalBias Dataset

We collect images of people with various professions, activities, and objects (PAO-EvalBias). For each concept in the lexicons, we use templates to construct one reference as well as two candidates containing the correct and incorrect gender, denoted as the good and bad captions respectively. Our approach involves pairing a gender from protected groups (man or woman) with a concept in professions, activities, or objects. We collect **91,640** images for PAO-EvalBias including üßë‚Äçüíº **88** professions, üèÉ‚Äç‚ôÄÔ∏è **52** activities, and üçé **39** objects.

<p align="center">
    <img src="assets/pao_eval_dataset.png?v=1&type=image"  width="90%;">
    <br>
    Figure 1: Example instances from PAO-EvalBias. 
</p>

Candidate and reference captions follow specific patterns described in the table below. The lexicon word is highlighted in **bold** in the **reference caption**, while the gender identification word is in <span style="color: blue;">blue</span> for a **good caption** and in <span style="color: red;">red</span> for a **bad caption**. A good caption maintains the same gender as the reference sentence, while a bad caption replaces the gender in the good caption with an incorrect gender. For example, in the image located at the top left corner featuring a male editor, the good caption reads ‚Äúa man who is an editor,‚Äù while the bad caption replaces ‚Äúman‚Äù with ‚Äúwoman‚Äù.

<p align="center">
    <img src="assets/caption_template.png?v=1&type=image"  width="90%;">
    <br>
    Figure 2: Caption patterns in PAO-EvalBias. 
</p>

### üì¶ Dataset Download

You can download the **images** dataset from https://drive.google.com/file/d/1Aiby0oakEMEOaoGnm8zZQIwwKDghMUj1/view?usp=drive_link.

You can find all **captions** for the images under the directory `captions`. Each entry is formatted as:

```json
  {
    "image_path": "../PAO-EvalBias-images/woman_activity/baking/woman_baking_4_1.jpg",
    "reference": "a photo of a woman who is baking",
    "good_caption": "a woman who is baking",
    "bad_caption": "a man who is baking"
  }
```

## ‚ú® Citation

If you find this work useful, please consider citing our paper as:

```
@inproceedings{qiu2023gender,
  title={Gender Biases in Automatic Evaluation Metrics for Image Captioning},
  author={Qiu, Haoyi and Dou, Zi-Yi and Wang, Tianlu and Celikyilmaz, Asli and Peng, Nanyun},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  rank={20},
  keyword={evaluation, fairness},
  paper_url={https://arxiv.org/abs/2305.14268},
  year={2023}
}
```
